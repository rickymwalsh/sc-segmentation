{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config_file import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config['gpu_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import commands\n",
    "from msct_image import Image\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinalcordtoolbox.deepseg_sc.cnn_models_3d import load_trained_model\n",
    "from generator import get_training_and_validation_generators\n",
    "from utils import fetch_data_files, visualize_data, normalize_data, load_3Dpatches, train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PARAMETERS FROM CONFIG FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['data_dict'] = pickle file containing a dictionary with at least the following keys: subject and contrast_foldname\n",
    "# This dict is load as a panda dataframe and used by the function utils.fetch_data_files\n",
    "# IMPORTANT NOTE: the testing dataset is not included in this dataframe\n",
    "DATA_PD = pd.read_pickle(config['data_dict'])\n",
    "# print(DATA_PD)\n",
    "DATA_FOLD = config[\"data_dir\"]  # where the preprocess data are stored\n",
    "MODEL_FOLD = config[\"path2save\"]  # where to store the trained models\n",
    "\n",
    "MEAN_TRAIN_T2, STD_TRAIN_T2 = 871.309, 557.916  # Mean and SD of the training dataset of the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT INPUT IMAGES INTO AN HDF5 FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len(DATA_PD.index)) # 80% of the dataset is used for the training \n",
    "idx_train = random.sample(range(len(DATA_PD.index)), len_train)\n",
    "idx_valid = [ii for ii in range(len(DATA_PD.index)) if ii not in idx_train] # the remaining images are used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_train)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_MASK')\n",
    "validation_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_valid)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_MASK')\n",
    "\n",
    "print(training_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT 3D PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted patches are stored as pickle files (one for training, one for validation).\n",
    "# If these files already exist, we load them directly (i.e. do not re run the patch extraction).\n",
    "pkl_train_fname = DATA_FOLD + 'lesion_train_data_t2.pkl'\n",
    "print(pkl_train_fname)\n",
    "if not os.path.isfile(pkl_train_fname):\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=config[\"patch_overlap\"]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    print(X_train.shape)\n",
    "    with open(pkl_train_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_train, y_train]), fp)\n",
    "else:\n",
    "    with open (pkl_train_fname, 'rb') as fp:\n",
    "        X_train, y_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_valid_fname = DATA_FOLD + 'lesion_valid_data_t2.pkl'\n",
    "print(pkl_valid_fname)\n",
    "\n",
    "if not os.path.isfile(pkl_valid_fname):\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    \n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    with open(pkl_valid_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_valid, y_valid]), fp)\n",
    "else:\n",
    "    with open (pkl_valid_fname, 'rb') as fp:\n",
    "        X_valid, y_valid = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Number of Training patches:\\n\\t' + str(X_train.shape[0])\n",
    "print 'Number of Validation patches:\\n\\t' + str(X_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = os.path.join(commands.getoutput('$SCT_DIR').split(': ')[2], 'data', 'deepseg_lesion_models', 't2_lesion.h5')\n",
    "model = load_trained_model(model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TRAINING AND VALIDATION GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=config[\"batch_size\"],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "print(train_generator,nb_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "print(validation_generator,nb_valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g, g_name in zip([train_generator, validation_generator], ['train_visu', 'valid_visu']):\n",
    "    print '\\n' + g_name\n",
    "    X_visu_, y_visu_ = g.next()\n",
    "    idx_random = random.randint(0, X_visu_.shape[-1])\n",
    "    visualize_data(X=X_visu_[0,0,:,:,idx_random], Y=y_visu_[0,0,:,:,idx_random])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN NET ---> Cell to change --> Change it to fine-tuning / transfer learning...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model=model,\n",
    "            path2save=config[\"path2save\"],\n",
    "            model_name=config[\"model_name\"],\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=config[\"n_epochs\"],\n",
    "            learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
